{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w5_practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1626438138725}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Laa7GeofREyP"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"weGiuGAzREyR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626438230528,"user_tz":-180,"elapsed":31793,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"c853add7-66a5-497a-f9c4-2655e99e4456"},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Selecting previously unselected package xvfb.\n","(Reading database ... 160815 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KXrQELkMREyS","executionInfo":{"status":"ok","timestamp":1626438232240,"user_tz":-180,"elapsed":1725,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVNA5u_BREyS"},"source":["A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."]},{"cell_type":"code","metadata":{"id":"eP08oYslREyS","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1626438237420,"user_tz":-180,"elapsed":1802,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"0d9558d2-2f4c-4180-9087-d99e39b0174c"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f3d9a2b14d0>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"O4KZCE71REyS"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"6Dn6ASADREyT"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"VZlApyYFREyT","executionInfo":{"status":"ok","timestamp":1626438279466,"user_tz":-180,"elapsed":5871,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORH3Gm6qREyU","executionInfo":{"status":"ok","timestamp":1626438346802,"user_tz":-180,"elapsed":223,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MagxCuI8SVLT","executionInfo":{"status":"ok","timestamp":1626438457976,"user_tz":-180,"elapsed":247,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["Dense?"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMZkYt84REyU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626438740041,"user_tz":-180,"elapsed":223,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"7a5c3840-039f-4a93-98dd-58684090c752"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# define network graph using raw TF, Keras, or any other library you prefer\n","network = Sequential()\n","network.add(Dense(32, activation='relu', input_shape=state_dim))\n","network.add(Dense(32, activation='relu'))\n","network.add(Dense(n_actions, activation='linear'))\n","# symbolic outputs of your network _before_ softmax\n","logits = network(ph_states)\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qbi4EuCBREyU","executionInfo":{"status":"ok","timestamp":1626438743332,"user_tz":-180,"elapsed":215,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCUSbb6_REyU","executionInfo":{"status":"ok","timestamp":1626438766333,"user_tz":-180,"elapsed":211,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lD4Y2KOdREyV"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"1QOvkdQvUJeB","executionInfo":{"status":"ok","timestamp":1626438934772,"user_tz":-180,"elapsed":234,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["np.random.choice?"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVLJWSCsREyV","executionInfo":{"status":"ok","timestamp":1626439061283,"user_tz":-180,"elapsed":237,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(n_actions,1, p=action_probs)[0]\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSKWTcCGREyV","executionInfo":{"status":"ok","timestamp":1626439062868,"user_tz":-180,"elapsed":222,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gja44IgvREyV"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"KMKBCxlWREyW","executionInfo":{"status":"ok","timestamp":1626439570626,"user_tz":-180,"elapsed":223,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["from collections import deque\n","def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cumulative_rewards = deque([rewards[-1]])\n","    for i in range(len(rewards)-2, -1, -1):\n","        cumulative_rewards.appendleft(rewards[i]+gamma*cumulative_rewards[0]) \n","    return cumulative_rewards"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyFAHkq1REyW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626439572522,"user_tz":-180,"elapsed":218,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"c1ac3be0-9011-4060-a44e-0d260e05cc98"},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"66qmOJE4REyW"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"oSvdXigpREyW","executionInfo":{"status":"ok","timestamp":1626439580939,"user_tz":-180,"elapsed":220,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"UywYVPEOREyX","executionInfo":{"status":"ok","timestamp":1626439818096,"user_tz":-180,"elapsed":219,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leZKDkugREyX"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"FAEHKvPzREyX","executionInfo":{"status":"ok","timestamp":1626439945440,"user_tz":-180,"elapsed":222,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","# compute entropy. Do not forget the sign!\n","entropy = -tf.reduce_sum(policy * log_policy, 1, name=\"entropy\")"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUMD3sbgREyX","executionInfo":{"status":"ok","timestamp":1626439946882,"user_tz":-180,"elapsed":238,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KoGjmFcREyX","executionInfo":{"status":"ok","timestamp":1626439950230,"user_tz":-180,"elapsed":219,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0fDDU8rREyX","executionInfo":{"status":"ok","timestamp":1626439952768,"user_tz":-180,"elapsed":219,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oim7ZRm9REyX"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"JDpxGSaFREyY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626440007473,"user_tz":-180,"elapsed":51951,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"d1887a79-d0a7-4b89-bc9d-f3d19874ce10"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":33,"outputs":[{"output_type":"stream","text":["mean reward: 30.350\n","mean reward: 56.850\n","mean reward: 162.910\n","mean reward: 216.870\n","mean reward: 472.550\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lBo-ilnMREyY"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"yMBAk0xKREyY","executionInfo":{"status":"ok","timestamp":1626440111099,"user_tz":-180,"elapsed":23637,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_pOo1lLREyY","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.63.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1626440111100,"user_tz":-180,"elapsed":6,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"a151bb75-83b0-45c0-e6e0-8ee1d68e0bef"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.63.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"u10RFIH_REyY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626440073128,"user_tz":-180,"elapsed":43685,"user":{"displayName":"Valentin Goryachev","photoUrl":"","userId":"06670255808124058379"}},"outputId":"36b3c03d-1e49-4dda-d7d4-a27dd9b62e75"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'iskander0697@gmail.com', 'yWtFZD9cdIkaykIA')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Your average reward is 798.59 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lksup0NtREyY"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}